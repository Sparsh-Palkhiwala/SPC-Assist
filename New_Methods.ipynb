{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","authorship_tag":"ABX9TyOo98haZlLqDueoG9PMS5Xe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8xV_EYlTXRJ2","executionInfo":{"status":"ok","timestamp":1725947607174,"user_tz":420,"elapsed":19319,"user":{"displayName":"Sparsh Devang Palkhiwala","userId":"15307326859011572794"}},"outputId":"0dd12ea2-7623-4add-cdd4-154f907fb10c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%pip install sentence-transformers scikit-learn numpy python-docx rouge_score"],"metadata":{"collapsed":true,"id":"TQkRoQZWYyuS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725947610696,"user_tz":420,"elapsed":1607,"user":{"displayName":"Sparsh Devang Palkhiwala","userId":"15307326859011572794"}},"outputId":"2f04800e-a581-448f-931f-ae74ef8cf3ad"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n","Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n","Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.0+cpu)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.6)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.7.24)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"]}]},{"cell_type":"code","source":["from docx import Document\n","\n","def load_document_from_file(file_path):\n","    if file_path.endswith('.txt'):\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            document = file.read()\n","    elif file_path.endswith('.docx'):\n","        doc = Document(file_path)\n","        document = '\\n'.join([para.text for para in doc.paragraphs])\n","    else:\n","        raise ValueError(\"Unsupported file format. Please provide a .txt or .docx file.\")\n","\n","    return document\n","\n","# Example usage:\n","file_path = '/content/drive/MyDrive/SPC_Assist/Contracts/New_contract.docx'  # Replace with your document path\n","document = load_document_from_file(file_path)\n"],"metadata":{"id":"982mhdyucmlN","executionInfo":{"status":"ok","timestamp":1725947614746,"user_tz":420,"elapsed":138,"user":{"displayName":"Sparsh Devang Palkhiwala","userId":"15307326859011572794"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","def preprocess_document(document):\n","    sentences = document.split('.')  # Simple sentence splitting by period\n","\n","    processed_sentences = []\n","    for sentence in sentences:\n","        # Remove special characters and extra whitespace, convert to lowercase\n","        sentence_cleaned = re.sub(r'[^\\w\\s]', '', sentence).strip().lower()\n","        processed_sentences.append(sentence_cleaned)\n","\n","    return [sentence for sentence in processed_sentences if sentence]  # Remove empty sentences\n","\n","# Preprocess the loaded document\n","sentences = preprocess_document(document)\n"],"metadata":{"id":"NwLXdnJPcu-3","executionInfo":{"status":"ok","timestamp":1725947614894,"user_tz":420,"elapsed":2,"user":{"displayName":"Sparsh Devang Palkhiwala","userId":"15307326859011572794"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","\n","# Define constants\n","MODEL_NAME = 'all-MiniLM-L12-v2'  # or use 'all-MiniLM-L12-v2' or 'all-mpnet-base-v2'\n","\n","# Load the SentenceTransformer model\n","model = SentenceTransformer(MODEL_NAME)\n","\n","# Custom Prompt Creation\n","prompt = \"Summarize the key elements and critical processes involved in the supply chain, including procurement, production, logistics, inventory management, and distribution. The summary should cover potential challenges, efficiencies, and strategic impacts, while also addressing any penalties, termination clauses, legal considerations, and intricate details that might affect operations. Ensure that the summary captures the essence of all sections, including risk management strategies, compliance with regulations, sustainability practices, and the implications of technological integration. The final summary should distill all pages of content into a comprehensive overview, retaining all critical points and nuances.\"\n","\n","# Encode the custom prompt\n","prompt_embedding = model.encode(prompt, convert_to_tensor=True)\n","\n","# Encode the preprocessed sentences\n","sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1vYyeGncxvz","outputId":"2efe3500-fbef-43de-bf58-e250b92905b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm, trange\n","/usr/local/lib/python3.10/dist-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from sentence_transformers import util\n","\n","def compute_similarity_and_rank(prompt_embedding, sentence_embeddings, sentences):\n","    # Compute cosine similarity between the prompt embedding and sentence embeddings\n","    similarities = util.pytorch_cos_sim(prompt_embedding, sentence_embeddings)[0]\n","\n","    # Move similarities tensor to CPU and convert to numpy array\n","    similarities = similarities.cpu().numpy()\n","\n","    # Sort the indices in descending order of similarity scores\n","    ranked_indices = np.argsort(-similarities).tolist()  # Negate similarities to get descending order\n","\n","    # Pair the ranked sentences with their similarity scores\n","    ranked_sentences = [(sentences[idx], similarities[idx]) for idx in ranked_indices]\n","\n","    return ranked_sentences\n","\n","# Compute similarities and rank sentences\n","ranked_sentences = compute_similarity_and_rank(prompt_embedding, sentence_embeddings, sentences)\n"],"metadata":{"id":"SsogrfP-c37r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adjust similarity scores with positional bias\n","\n","def apply_positional_bias(ranked_sentences, decay_factor=0.9):\n","    for i, (sentence, score) in enumerate(ranked_sentences):\n","        positional_weight = decay_factor ** i\n","        ranked_sentences[i] = (sentence, score * positional_weight)\n","\n","    return sorted(ranked_sentences, key=lambda x: x[1], reverse=True)\n","\n","# Apply positional bias to ranked sentences\n","ranked_sentences_with_bias = apply_positional_bias(ranked_sentences)"],"metadata":{"id":"iRD5M5efeS0n"},"execution_count":null,"outputs":[]},{"source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","def remove_redundancy(ranked_sentences, sentence_embeddings, threshold=0.8):\n","    \"\"\"\n","    Remove redundant sentences based on cosine similarity between top-ranked sentences.\n","    \"\"\"\n","    selected_sentences = []\n","    selected_embeddings = []\n","\n","    for sentence, score in ranked_sentences:\n","        if selected_sentences:\n","            # Compute similarity with already selected sentences\n","            # Move the tensor to CPU using .cpu() and then convert it to a NumPy array using .numpy()\n","            similarities = cosine_similarity([sentence_embeddings[sentences.index(sentence)].cpu().numpy()], selected_embeddings)\n","            if np.max(similarities) < threshold:\n","                selected_sentences.append(sentence)\n","                selected_embeddings.append(sentence_embeddings[sentences.index(sentence)].cpu().numpy()) # Move the tensor to CPU using .cpu() and then convert it to a NumPy array using .numpy()\n","        else:\n","            selected_sentences.append(sentence)\n","            # Move the tensor to CPU using .cpu() and then convert it to a NumPy array using .numpy()\n","            selected_embeddings.append(sentence_embeddings[sentences.index(sentence)].cpu().numpy())\n","\n","        if len(selected_sentences) >= 100:  # Select top 50 sentences\n","            break\n","\n","    return selected_sentences\n","\n","# Remove redundant sentences\n","final_sentences = remove_redundancy(ranked_sentences_with_bias, sentence_embeddings)"],"cell_type":"code","metadata":{"id":"38NCFQnlBs3l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def boost_by_keywords(sentences, document, boost_factor=1.2):\n","    \"\"\"\n","    Boost the scores of sentences containing key terms.\n","    \"\"\"\n","    vectorizer = TfidfVectorizer()\n","    tfidf_matrix = vectorizer.fit_transform([document])\n","    feature_names = vectorizer.get_feature_names_out()\n","\n","    keywords = [feature_names[idx] for idx in np.argsort(np.array(tfidf_matrix.sum(axis=0)).flatten())[::-1][:5]]\n","\n","    boosted_sentences = []\n","    for sentence in sentences:\n","        score_boost = boost_factor if any(keyword in sentence for keyword in keywords) else 1.0\n","        boosted_sentences.append((sentence, score_boost))\n","\n","    return boosted_sentences\n","\n","# Boost the scores of sentences by keywords\n","boosted_sentences = boost_by_keywords(final_sentences, document)\n"],"metadata":{"id":"MvnjwBW6eWu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_summary(ranked_sentences, num_sentences=50):\n","    \"\"\"\n","    Generate a summary by selecting the top-ranked sentences.\n","\n","    Parameters:\n","    ranked_sentences (list): A list of (sentence, score) tuples.\n","    num_sentences (int): The number of top-ranked sentences to include in the summary.\n","\n","    Returns:\n","    str: The generated summary.\n","    \"\"\"\n","    # Select the top N sentences based on ranking\n","    selected_sentences = [sentence for sentence, score in ranked_sentences[:num_sentences]]\n","\n","    # Join the selected sentences to form the summary\n","    summary = ' '.join(selected_sentences)\n","\n","    return summary\n","\n","summary = generate_summary(boosted_sentences)\n","print(\"Summary:\\n\", summary)"],"metadata":{"id":"WQeyYIIuh1Ah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sliding_window_summarization(document, window_size=500, overlap=100):\n","    \"\"\"\n","    Summarize the document by applying summarization over sliding windows.\n","\n","    Parameters:\n","    document (str): The full document as a string.\n","    window_size (int): The size of each window in characters.\n","    overlap (int): The overlap between consecutive windows.\n","\n","    Returns:\n","    str: The combined summary from all windows.\n","    \"\"\"\n","    summary = \"\"\n","    start = 0\n","    end = window_size\n","\n","    # Iterate through document in windows\n","    while start < len(document):\n","        window_text = document[start:end]\n","        sentences = tokenize_sentences(window_text)\n","        window_summary = summarization_pipeline(sentences, prompt_embedding, sentence_embeddings, ...)\n","\n","        summary += window_summary + \" \"  # Add the window summary to the final summary\n","\n","        # Move the window\n","        start = end - overlap\n","        end = start + window_size\n","\n","    return summary\n","\n","summary = generate_summary(boosted_sentences)\n","print(\"Summary:\\n\", summary)"],"metadata":{"id":"upxFHvILeYKU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","from rouge_score import rouge_scorer\n","import numpy as np\n","\n","# Simulate a reference summary for ROUGE evaluation (this would be the ground truth summary)\n","reference_summary = \"The Master Subcontract Agreement between W.E. O'Neil Construction Co. of Arizona and Blind Ideas, entered into on October 30, 2023, sets forth terms for future projects. It specifies that the Subcontractor will provide all necessary labor, equipment, materials, and services, and is required to comply with federal, state, and local laws and regulations. The Subcontractor must ensure a safe working environment and the safety of their employees. Payment is governed by the Prime Contract, with retention possible for reasons such as defective work or non-payment to sub-subcontractors or suppliers. Disputes are to be resolved through direct discussions and mediation, and if necessary, arbitration according to Judicial Arbitration and Mediation Services (JAMS) rules. The prevailing party is entitled to recover legal costs, including attorney and expert fees.The Subcontractor is required to maintain various insurance policies, including Worker's Compensation, Commercial General Liability, and Automobile Liability, and must indemnify the General Contractor against any claims arising from non-compliance with insurance or safety requirements. Payment is made within seven days of the General Contractor receiving payment from the Owner, contingent on the Subcontractor’s submission of required documentation such as invoices, waivers of lien, certified payroll records, and safety compliance statements. The Subcontractor’s warranty covers all materials and labor, and any defects must be corrected at their own expense. Final payment is contingent on fulfilling all Subcontract obligations, including submitting punch lists and warranties.The agreement also prohibits the Subcontractor from assigning or transferring work without written consent. Modifications must be in writing and signed by both parties, and the Subcontractor must adhere to prevailing wage laws and other employment regulations. The agreement allows for termination by either party for cause or convenience, subject to certain conditions. Failure to comply with safety regulations or other obligations may result in withheld payments or backcharges. Additional provisions address indemnification, confidentiality, governing law, and force majeure. This summary provides a general overview of the agreement’s terms, and the full document should be consulted for detailed legal implications.\"\n","\n","# Function to compute ROUGE score\n","def compute_rouge(predicted_summary, reference_summary):\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n","    scores = scorer.score(reference_summary, predicted_summary)\n","    return scores['rouge1'].fmeasure, scores['rougeL'].fmeasure\n","\n","# Function to perform the entire summarization process\n","def summarization_pipeline(sentences, prompt_embedding, sentence_embeddings, positional_decay, redundancy_threshold, boost_factor):\n","    \"\"\"\n","    Pipeline to generate summaries using the given hyperparameters.\n","    \"\"\"\n","    # Step 1: Compute similarity and rank\n","    ranked_sentences = compute_similarity_and_rank(prompt_embedding, sentence_embeddings, sentences)\n","\n","    # Step 2: Apply positional bias\n","    ranked_sentences_with_bias = apply_positional_bias(ranked_sentences, decay_factor=positional_decay)\n","\n","    # Step 3: Remove redundancy\n","    final_sentences = remove_redundancy(ranked_sentences_with_bias, sentence_embeddings, threshold=redundancy_threshold)\n","\n","    # Step 4: Boost by keywords\n","    boosted_sentences = boost_by_keywords(final_sentences, document, boost_factor=boost_factor)\n","\n","    # Step 5: Generate the final summary\n","    summary = generate_summary(boosted_sentences)\n","\n","    return summary\n","\n","# Define hyperparameter search space\n","positional_decay_values = [0.8, 0.85, 0.9, 0.95, 1.0]\n","redundancy_threshold_values = [0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n","boost_factor_values = [1.0, 1.1, 1.2, 1.3, 1.4, 1.5]\n","\n","# Grid search for hyperparameter tuning\n","def hyperparameter_tuning(sentences, prompt_embedding, sentence_embeddings):\n","    best_hyperparams = None\n","    best_rouge_score = 0\n","\n","    # Loop over all combinations of hyperparameters\n","    for positional_decay in positional_decay_values:\n","        for redundancy_threshold in redundancy_threshold_values:\n","            for boost_factor in boost_factor_values:\n","                # Generate summary for this combination\n","                generated_summary = summarization_pipeline(\n","                    sentences, prompt_embedding, sentence_embeddings,\n","                    positional_decay, redundancy_threshold, boost_factor\n","                )\n","\n","                # Compute ROUGE score\n","                rouge1_f1, rougeL_f1 = compute_rouge(generated_summary, reference_summary)\n","\n","                # Average ROUGE score\n","                avg_rouge_score = (rouge1_f1 + rougeL_f1) / 2\n","\n","                # Check if this is the best score\n","                if avg_rouge_score > best_rouge_score:\n","                    best_rouge_score = avg_rouge_score\n","                    best_hyperparams = {\n","                        'positional_decay': positional_decay,\n","                        'redundancy_threshold': redundancy_threshold,\n","                        'boost_factor': boost_factor\n","                    }\n","\n","                print(f\"positional_decay={positional_decay}, redundancy_threshold={redundancy_threshold}, boost_factor={boost_factor} -> ROUGE1: {rouge1_f1}, ROUGE-L: {rougeL_f1}\")\n","\n","    return best_hyperparams, best_rouge_score\n","\n","# Perform hyperparameter tuning\n","best_hyperparams, best_rouge_score = hyperparameter_tuning(sentences, prompt_embedding, sentence_embeddings)\n","\n","print(f\"Best Hyperparameters: {best_hyperparams}\")\n","print(f\"Best ROUGE Score: {best_rouge_score}\")\n"],"metadata":{"id":"PT9ianfzM7-k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Maybe use another LLM to process an abstractive summary\n"],"metadata":{"id":"5acDnXmWjIcz"}},{"cell_type":"code","source":["import re\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","# Summarize using T5 with a custom prompt\n","def summarize_with_t5(custom_prompt, max_length=3000, output_file=\"summary.txt\"):\n","    # Load T5 model and tokenizer\n","    model_name = \"t5-small\"  # You can also try \"t5-base\" or \"t5-large\"\n","    tokenizer = T5Tokenizer.from_pretrained(model_name)\n","    model = T5ForConditionalGeneration.from_pretrained(model_name)\n","\n","    # Prepare input with the custom prompt\n","    input_text = f\"{custom_prompt} The important ranked statements are :{boosted_sentences} use this to make a comprehensice summary of : {sentences}\"\n","\n","    # Tokenize the input text\n","    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=4096, truncation=False)\n","\n","    # Generate the summary\n","    summary_ids = model.generate(inputs, max_length=max_length, min_length=30, length_penalty=2.0, num_beams=8, early_stopping=True)\n","    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","    # Save the summary to a text file\n","    with open(output_file, \"w\") as file:\n","        file.write(summary)\n","\n","    return summary\n","\n","# Example usage\n","custom_prompt = \"Summarize the key elements and critical processes involved in the supply chain, including procurement, production, logistics, inventory management, and distribution. The summary should cover potential challenges, efficiencies, and strategic impacts, while also addressing any penalties, termination clauses, legal considerations, and intricate details that might affect operations. Ensure that the summary captures the essence of all sections, including risk management strategies, compliance with regulations, sustainability practices, and the implications of technological integration. The final summary should distill all pages of content into a comprehensive overview, retaining all critical points and nuances.\"\n","summary = summarize_with_t5(custom_prompt)\n","print(summary)\n"],"metadata":{"id":"rZGhJYPsNjaM"},"execution_count":null,"outputs":[]}]}